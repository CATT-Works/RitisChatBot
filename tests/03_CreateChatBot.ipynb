{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import openai\n",
    "import config\n",
    "\n",
    "\n",
    "openai.api_key  = config.OPENAI_KEY\n",
    "os.environ['OPENAI_API_KEY'] = config.OPENAI_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_NAME = \"gpt-4\"\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=LLM_NAME, temperature=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to experiment on `LangChain plus platform`:\n",
    "\n",
    " * Go to [langchain plus platform](https://www.langchain.plus/) and sign up\n",
    " * Create an api key from your account's settings\n",
    " * Use this api key in the code below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = config.LANGCHAIN_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "persist_directory = '../chroma'\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "# template = \"\"\"\n",
    "# The following pieces of texts contain a RITIS tool name follwed by it's description.\n",
    "# Use these texts to addres the question and give the correct answer. \n",
    "# If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "# Use four sentences maximum. Keep the answer as concise as possible. \n",
    "# {context}\n",
    "# Question: {question}\n",
    "# Helpful Answer:\"\"\"\n",
    "\n",
    "template = \"\"\"\n",
    "The following pieces of texts contain framgents from RITIS tutorials.\n",
    "Use these texts to addres the question and give the correct answer. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use ten sentences maximum. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.memory import ConversationBufferMemory\n",
    "# memory = ConversationBufferMemory(\n",
    "#     memory_key=\"chat_history\",\n",
    "#     return_messages=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "question = \"What tool can I use to rank bottlenecks?\"\n",
    "qa= RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a chatbot that works on your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/wav2vec2-base-960h and revision 55bb623 (https://huggingface.co/facebook/wav2vec2-base-960h).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "audio_model = pipeline(\"automatic-speech-recognition\")\n",
    "\n",
    "def transcribe_audio(mic):\n",
    "\n",
    "    transcription = audio_model(mic)[\"text\"]\n",
    "    return transcription\n",
    "\n",
    "\n",
    "def ask_question(question = None, mic = None):\n",
    "    if mic is None:\n",
    "        input_text = question\n",
    "    else:\n",
    "        input_text = transcribe_audio(mic=mic)\n",
    "        \n",
    "    result = qa({\"query\": input_text})\n",
    "    \n",
    "    res = \"\"\"\n",
    "    Question: {}\\n\n",
    "    Answer: {}\\n\n",
    "    \"\"\".format(input_text, result['result'])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../app_description.html', 'r') as f:\n",
    "    app_description = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38268/818764809.py:4: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  gr.inputs.Textbox(label=\"Question\", optional=True),\n",
      "/tmp/ipykernel_38268/818764809.py:4: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  gr.inputs.Textbox(label=\"Question\", optional=True),\n",
      "/tmp/ipykernel_38268/818764809.py:4: GradioDeprecationWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  gr.inputs.Textbox(label=\"Question\", optional=True),\n",
      "/tmp/ipykernel_38268/818764809.py:5: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  gr.Audio(source=\"microphone\", type=\"filepath\", optional=True),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:8891\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:8891/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    fn=ask_question,\n",
    "    inputs=[\n",
    "        gr.inputs.Textbox(label=\"Question\", optional=True),\n",
    "        gr.Audio(source=\"microphone\", type=\"filepath\", optional=True),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    \n",
    "    title=\"RITIS chatbot - v 0.1\",  \n",
    "    description=app_description,\n",
    "    \n",
    "    \n",
    "    examples = [\n",
    "        [\"How can I use congestion scan?\", None],\n",
    "        \n",
    "        [\"What tool can I use to rank bottlenecks?\", None],\n",
    "        [\"how can I ask for CO2 emissions in College Park\", None],\n",
    "        [\"How to estimate the delay costs\", None],\n",
    "        [\"I need to analyze carbon dioxide emissions \" \\\n",
    "            \"in 2020 at I-95 corridor and check if there \" \\\n",
    "            \"were any anomalies. Can I do it with RITIS?\", None],\n",
    "    ]\n",
    ").launch(server_port= 8891)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
